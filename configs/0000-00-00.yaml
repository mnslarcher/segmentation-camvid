seed: 42

categories:
  ignore: 0
  sky: 1
  building: 2
  pole: 3
  road: 4
  pavement: 5
  tree: 6
  signsymbol: 7
  fence: 8
  car: 9
  pedestrian: 10
  bicyclist: 11
  unlabelled: 12

model:
  model:
    type: segmentation_models_pytorch.DeepLabV3Plus
    encoder_weights: noisy-student
    encoder_name: timm-efficientnet-b3
    encoder_depth: 5
    encoder_output_stride: 16
    decoder_channels: 256
    decoder_atrous_rates:
      - 12
      - 24
      - 36
    in_channels: 3
    activation: null
    upsampling: 4
    aux_params: null

  optimizer:
    type: torch.optim.SGD
    lr: 0.01
    momentum: 0.9
    weight_decay: 0
    nesterov: false

  loss:
    type: pytorch_toolbelt.losses.DiceLoss
    mode: multiclass
    classes: null
    log_loss: false
    from_logits: true
    smooth: 0.0
    ignore_index: 0  # index of the category of pixels to be ignored

  scheduler:
    scheduler:
      type: torch.optim.lr_scheduler.OneCycleLR
      max_lr: 0.2
      total_steps: 30000

    interval: step

    frequency: 1

trainer:
  trainer:
    type: pytorch_lightning.Trainer
    gpus: -1
    gradient_clip_val: 0.5
    log_gpu_memory: true
    max_steps: 30000

  callbacks:
    model_checkpoint:
      type: pytorch_lightning.callbacks.ModelCheckpoint
      filename: "{epoch}-{step}-{val_loss:.2f}"
      monitor: val_loss
      verbose: true
      mode: min
      save_top_k: 1

    learning_rate_monitor:
      type: pytorch_lightning.callbacks.LearningRateMonitor
      log_momentum: true

    early_stopping:
      type: pytorch_lightning.callbacks.EarlyStopping
      monitor: val_loss
      min_delta: 0.0
      patience: 100
      mode: min

  logger:
    type: bool
    x: true

data:
  data:
    data_dir: ./data/CamVid/preproc/
    batch_size: 8
    num_workers: 16
    pin_memory: true

  transforms:
      train:
        transform:
          __class_fullname__: albumentations.core.composition.Compose
          transforms:
            - __class_fullname__: albumentations.augmentations.transforms.LongestMaxSize
              max_size: 320
            - __class_fullname__: albumentations.augmentations.transforms.PadIfNeeded
              min_height: 320
              min_width: 320
              border_mode: 0  # cv2.BORDER_CONSTANT
              value: 0.0
              mask_value: 0.0
            - __class_fullname__: albumentations.augmentations.transforms.HorizontalFlip
              p: 0.5
            - __class_fullname__: albumentations.augmentations.transforms.Normalize
            - __class_fullname__: albumentations.pytorch.transforms.ToTensorV2
              transpose_mask: true

      val:
        transform:
          __class_fullname__: albumentations.core.composition.Compose
          transforms:
            - __class_fullname__: albumentations.augmentations.transforms.LongestMaxSize
              max_size: 320
            - __class_fullname__: albumentations.augmentations.transforms.PadIfNeeded
              min_height: 320
              min_width: 320
              border_mode: 0  # cv2.BORDER_CONSTANT
              value: 0
              mask_value: 0
            - __class_fullname__: albumentations.augmentations.transforms.Normalize
            - __class_fullname__: albumentations.pytorch.transforms.ToTensorV2
              transpose_mask: true
      test:
        transform:
          __class_fullname__: albumentations.core.composition.Compose
          transforms:
            - __class_fullname__: albumentations.augmentations.transforms.LongestMaxSize
              max_size: 320
            - __class_fullname__: albumentations.augmentations.transforms.PadIfNeeded
              min_height: 320
              min_width: 320
              border_mode: 0  # cv2.BORDER_CONSTANT
              value: 0
              mask_value: 0
            - __class_fullname__: albumentations.augmentations.transforms.Normalize
            - __class_fullname__: albumentations.pytorch.transforms.ToTensorV2
              transpose_mask: true
